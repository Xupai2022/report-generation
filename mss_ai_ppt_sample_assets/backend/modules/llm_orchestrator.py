from __future__ import annotations

import json
import logging
import re
import time
from typing import Any, Dict, List, Optional

from openai import OpenAI, APIError, RateLimitError, APIConnectionError

from mss_ai_ppt_sample_assets.backend import config
from mss_ai_ppt_sample_assets.backend.models.slidespec import (
    SlideSpecV2, create_empty_slidespec_v2
)
from mss_ai_ppt_sample_assets.backend.models.templates import (
    TemplateDescriptorV2, PlaceholderDefinition
)
from mss_ai_ppt_sample_assets.backend.models.inputs import TenantInput
from mss_ai_ppt_sample_assets.backend.modules.template_loader import TemplateRepository

# Configure logging
logger = logging.getLogger(__name__)


class LLMGenerationError(Exception):
    """Error during LLM content generation"""
    pass


class LLMOrchestratorV2:
    """V2 Orchestrator for AI-driven content generation.

    Key differences from V1:
    - Takes raw TenantInput directly, no pre-processing
    - Uses placeholder-based AI instructions from template
    - Generates content based on ai_instruction fields
    - Validates only key numerical fields
    """

    def __init__(self, template_repo: Optional[TemplateRepository] = None):
        self.template_repo = template_repo or TemplateRepository()
        self.client: Optional[OpenAI] = None

        if config.settings.enable_llm:
            try:
                client_kwargs = {"api_key": config.settings.openai_api_key}
                if config.settings.openai_base_url:
                    client_kwargs["base_url"] = config.settings.openai_base_url
                self.client = OpenAI(**client_kwargs)
                logger.info("OpenAI client initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                raise LLMGenerationError(f"OpenAI client initialization failed: {e}") from e

    def _get_nested(self, data: Any, path: str) -> Any:
        """Get nested value from dict or TenantInput using dot notation path."""
        if not path:
            return None

        # Handle TenantInput by getting its raw data
        if hasattr(data, 'raw'):
            current = data.raw
        elif isinstance(data, dict):
            current = data
        else:
            return None

        for part in path.split('.'):
            if isinstance(current, dict):
                current = current.get(part)
            elif isinstance(current, list) and part.isdigit():
                idx = int(part)
                current = current[idx] if idx < len(current) else None
            else:
                return None
            if current is None:
                return None
        return current

    def _resolve_format_path(self, data: Dict[str, Any], path: str) -> Any:
        """Resolve a dotted path in data, handling .length for lists."""
        parts = path.split('.')
        current = data

        for i, part in enumerate(parts):
            if part == "length" and isinstance(current, list):
                return len(current)

            if isinstance(current, dict):
                current = current.get(part)
            elif isinstance(current, list) and part.isdigit():
                idx = int(part)
                current = current[idx] if idx < len(current) else None
            else:
                return None

            if current is None:
                return None

        return current

    def _format_template_string(self, template: str, data: Dict[str, Any]) -> str:
        """Format a template string with custom path resolution supporting .length."""
        import re

        def replace_placeholder(match):
            path = match.group(1)
            value = self._resolve_format_path(data, path)
            if value is None:
                return match.group(0)  # Keep original if not found
            return str(value)

        # Find all {path} patterns and replace them
        result = re.sub(r'\{([^}]+)\}', replace_placeholder, template)
        return result

    def _format_value(self, value: Any, placeholder: PlaceholderDefinition) -> str:
        """Format a value according to placeholder definition."""
        if value is None:
            return placeholder.default or ""

        # Apply transform
        if placeholder.transform:
            if placeholder.transform == "uppercase":
                value = str(value).upper()
            elif placeholder.transform == "lowercase":
                value = str(value).lower()
            elif placeholder.transform == "percent":
                if isinstance(value, (int, float)):
                    value = f"{round(value * 100)}%"

        # Handle list values FIRST (before format check)
        if isinstance(value, list):
            if placeholder.format and "{" in placeholder.format:
                # Format each list item using the format template
                formatted_items = []
                for item in value:
                    if isinstance(item, dict):
                        try:
                            formatted_items.append(self._format_template_string(placeholder.format, item))
                        except Exception:
                            formatted_items.append(str(item))
                    else:
                        formatted_items.append(str(item))
                return "\n".join(f"â€¢ {item}" for item in formatted_items)
            elif placeholder.format == "join_comma":
                return ", ".join(str(v) for v in value)
            else:
                return "\n".join(f"â€¢ {str(v)}" for v in value)

        # Apply format template for non-list values
        if placeholder.format:
            if placeholder.format == "percent":
                if isinstance(value, (int, float)):
                    return f"{round(value * 100)}%"
            elif placeholder.format == "join_comma":
                return str(value)
            elif "{" in placeholder.format:
                # Template format like "{value}å°æ—¶" or "{start} ~ {end}"
                if isinstance(value, dict):
                    try:
                        return self._format_template_string(placeholder.format, value)
                    except Exception:
                        pass
                else:
                    try:
                        return placeholder.format.format(value=value)
                    except (KeyError, ValueError):
                        pass

        return str(value)

    def _extract_chart_data(
        self,
        tenant_input: TenantInput,
        chart_config: Dict[str, Any],
        chart_type: str
    ) -> Dict[str, Any]:
        """Extract and format chart data from tenant input.

        Args:
            tenant_input: Raw tenant input
            chart_config: Chart configuration from placeholder definition
            chart_type: 'bar_chart' or 'pie_chart'

        Returns:
            Formatted chart data ready for rendering
        """
        data_source = chart_config.get('data_source')
        if not data_source:
            logger.warning(f"Chart config missing data_source")
            return {}

        # Get data from tenant input
        source_data = self._get_nested(tenant_input, data_source)
        if not source_data:
            logger.warning(f"No data found at {data_source}")
            return {}

        result = {
            'position': chart_config.get('position')
        }

        if chart_type == 'bar_chart':
            # Expect source_data to have 'labels' and 'values' or similar structure
            x_field = chart_config.get('x_field', 'labels')
            y_field = chart_config.get('y_field', 'values')

            if isinstance(source_data, dict):
                # Direct dict format: {"labels": [...], "values": [...]}
                categories = source_data.get(x_field, [])
                values = source_data.get(y_field, [])

                result['categories'] = categories
                result['series'] = [{'name': chart_config.get('series_name', 'å‘Šè­¦æ•°'), 'values': values}]
            elif isinstance(source_data, list):
                # List of objects format: [{"category": ..., "count": ...}, ...]
                categories = []
                values = []
                for item in source_data:
                    if isinstance(item, dict):
                        cat_value = item.get(x_field)
                        val_value = item.get(y_field)
                        if cat_value is not None:
                            categories.append(cat_value)
                            values.append(val_value if val_value is not None else 0)

                if categories:
                    result['categories'] = categories
                    result['series'] = [{'name': chart_config.get('series_name', 'å‘Šè­¦æ•°'), 'values': values}]
                else:
                    logger.warning(f"Bar chart data source {data_source} list has no valid items with fields {x_field}/{y_field}")
                    return {}
            else:
                logger.warning(f"Bar chart data source {data_source} is neither dict nor list")
                return {}

        elif chart_type == 'pie_chart':
            # Expect source_data to be a dict like {'high': 52, 'medium': 473, 'low': 816}
            if isinstance(source_data, dict):
                # Convert dict to categories and values
                categories = []
                values = []

                # Map severity levels to Chinese names
                severity_map = chart_config.get('category_map', {
                    'critical': 'ä¸¥é‡',
                    'high': 'é«˜å±',
                    'medium': 'ä¸­å±',
                    'low': 'ä½å±',
                    'info': 'ä¿¡æ¯'
                })

                for key, value in source_data.items():
                    # Use mapped name if available, otherwise use key
                    category_name = severity_map.get(key, key)
                    categories.append(category_name)
                    values.append(value)

                result['categories'] = categories
                result['values'] = values
            else:
                logger.warning(f"Pie chart data source {data_source} is not a dict")
                return {}

        return result

    def _extract_table_data(
        self,
        tenant_input: TenantInput,
        table_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract and format table data from tenant input.

        Args:
            tenant_input: Raw tenant input
            table_config: Table configuration from placeholder definition

        Returns:
            Formatted table data ready for rendering
        """
        data_source = table_config.get('data_source')
        columns_config = table_config.get('columns', [])

        if not data_source or not columns_config:
            logger.warning(f"Table config missing data_source or columns")
            return {}

        # Get data from tenant input
        source_data = self._get_nested(tenant_input, data_source)
        if not source_data or not isinstance(source_data, list):
            logger.warning(f"No list data found at {data_source}")
            return {}

        # Extract headers
        headers = [col.get('header', '') for col in columns_config]

        # Extract rows
        rows = []
        max_rows = table_config.get('max_rows', 10)
        for item in source_data[:max_rows]:
            if isinstance(item, dict):
                row = []
                for col in columns_config:
                    field_name = col.get('field', '')
                    value = item.get(field_name, '')

                    # Format value based on column config
                    if col.get('format') == 'percent' and isinstance(value, (int, float)):
                        value = f"{int(value * 100)}%"

                    row.append(value)
                rows.append(row)

        return {
            'headers': headers,
            'rows': rows,
            'position': table_config.get('position')
        }

    def _extract_data_placeholders(
        self,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2,
    ) -> Dict[str, Dict[str, Any]]:
        """Extract all non-AI placeholders from input data.

        Returns:
            Dict[slide_key, Dict[token, value]]
        """
        result: Dict[str, Dict[str, Any]] = {}

        # Calculate derived values
        incidents = tenant_input.get("incidents", []) or []
        incidents_high_count = len([i for i in incidents if i.get("severity") == "high"])
        incidents_count = len(incidents)

        # Add computed values to a lookup dict
        computed = {
            "incidents.length": incidents_count,
            "incidents.high_count": incidents_high_count,
            "incidents_count": incidents_count,
            "incidents_high_count": incidents_high_count,
        }

        for slide_key, token, placeholder in template.get_data_placeholders():
            if slide_key not in result:
                result[slide_key] = {}

            # Handle chart placeholders
            if placeholder.type in ('bar_chart', 'pie_chart') and placeholder.chart_config:
                chart_data = self._extract_chart_data(
                    tenant_input,
                    placeholder.chart_config,
                    placeholder.type
                )
                result[slide_key][token] = chart_data

            # Handle native table placeholders
            elif placeholder.type == 'native_table' and placeholder.table_config:
                table_data = self._extract_table_data(
                    tenant_input,
                    placeholder.table_config
                )
                result[slide_key][token] = table_data

            # Handle regular text placeholders
            elif placeholder.default and not placeholder.source:
                result[slide_key][token] = placeholder.default
            elif placeholder.source:
                # Check computed values first
                if placeholder.source in computed:
                    value = computed[placeholder.source]
                else:
                    value = self._get_nested(tenant_input, placeholder.source)
                result[slide_key][token] = self._format_value(value, placeholder)
            else:
                result[slide_key][token] = ""

        return result

    def _build_system_prompt(self, template: TemplateDescriptorV2) -> str:
        """Build the system prompt for AI generation."""
        audience_desc = "ç®¡ç†å±‚ï¼ˆéæŠ€æœ¯èƒŒæ™¯ï¼‰" if template.audience == "management" else "æŠ€æœ¯å›¢é˜Ÿï¼ˆå®‰å…¨å·¥ç¨‹å¸ˆï¼‰"

        return f"""ä½ æ˜¯ä¸€ä½èµ„æ·±å®‰å…¨åˆ†æå¸ˆï¼Œæ­£åœ¨ä¸ºå®¢æˆ·æ’°å†™MSSï¼ˆæ‰˜ç®¡å®‰å…¨æœåŠ¡ï¼‰æœˆåº¦å®‰å…¨æŠ¥å‘Šã€‚

## ä½ çš„è§’è‰²
- ä½ æ˜¯å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„å¨èƒåˆ†æå’Œå®‰å…¨è¿è¥ç»éªŒ
- ä½ æ“…é•¿ä»æ•°æ®ä¸­å‘ç°å®‰å…¨è¶‹åŠ¿å’Œæ´å¯Ÿ
- ä½ èƒ½ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ’°å†™å®‰å…¨æŠ¥å‘Š

## æŠ¥å‘Šå—ä¼—
æœ¬æŠ¥å‘Šé¢å‘ï¼š{audience_desc}

## å…³é”®è¦æ±‚
1. **æ•°æ®å‡†ç¡®æ€§**ï¼šæ‰€æœ‰å¼•ç”¨çš„æ•°å­—å¿…é¡»ä¸è¾“å…¥æ•°æ®å®Œå…¨ä¸€è‡´ï¼Œç»ä¸èƒ½ç¼–é€ æ•°æ®
2. **æ·±åº¦åˆ†æ**ï¼šä¸è¦åªæ˜¯ç®€å•ç½—åˆ—æ•°æ®ï¼Œè¦æä¾›æœ‰æ´å¯ŸåŠ›çš„åˆ†æå’Œè§£è¯»
3. **å…·ä½“å»ºè®®**ï¼šæ•´æ”¹å»ºè®®å¿…é¡»å…·ä½“å¯æ‰§è¡Œï¼Œé¿å…æ³›æ³›è€Œè°ˆ
4. **è¯­è¨€é£æ ¼**ï¼šä½¿ç”¨ä¸­æ–‡ï¼Œç®€æ´ä¸“ä¸šï¼Œé€‚åˆ{audience_desc}é˜…è¯»
5. **æ ¼å¼è¦æ±‚**ï¼šä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›å†…å®¹

## è¾“å‡ºæ ¼å¼
ä½ å¿…é¡»è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "slides": [
    {{
      "slide_key": "slide_key_here",
      "placeholders": {{
        "TOKEN_NAME": "ç”Ÿæˆçš„å†…å®¹"
      }}
    }}
  ]
}}
"""

    def _build_user_prompt(
        self,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2
    ) -> str:
        """Build the user prompt with data and AI instructions."""
        tenant = tenant_input.get("tenant", {})
        period = tenant_input.get("period", {})

        prompt_parts = [
            "## å®¢æˆ·ä¿¡æ¯",
            f"- å®¢æˆ·åç§°ï¼š{tenant.get('name', '')}",
            f"- è¡Œä¸šï¼š{tenant.get('industry', '')}",
            f"- åœ°åŒºï¼š{tenant.get('region', '')}",
            "",
            "## æŠ¥å‘Šå‘¨æœŸ",
            f"- å¼€å§‹æ—¥æœŸï¼š{period.get('start', '')}",
            f"- ç»“æŸæ—¥æœŸï¼š{period.get('end', '')}",
            "",
            "## å®‰å…¨æ•°æ®",
            "```json",
            json.dumps(tenant_input.raw, ensure_ascii=False, indent=2),
            "```",
            "",
            "## éœ€è¦ç”Ÿæˆçš„å†…å®¹",
            "",
        ]

        # Add AI placeholder instructions
        ai_placeholders = template.get_ai_placeholders()
        current_slide = None

        for slide_key, token, placeholder in ai_placeholders:
            if slide_key != current_slide:
                # Find slide title
                for slide in template.slides:
                    if slide.slide_key == slide_key:
                        prompt_parts.append(f"### Slide: {slide.title} ({slide_key})")
                        break
                current_slide = slide_key

            # Add placeholder instruction
            constraints = []
            if placeholder.max_length:
                constraints.append(f"æœ€å¤š{placeholder.max_length}å­—")
            if placeholder.max_items:
                constraints.append(f"æœ€å¤š{placeholder.max_items}æ¡")
            if placeholder.max_chars_per_item:
                constraints.append(f"æ¯æ¡æœ€å¤š{placeholder.max_chars_per_item}å­—")

            constraint_str = f" ({', '.join(constraints)})" if constraints else ""

            prompt_parts.append(f"\n**{token}**{constraint_str}")
            prompt_parts.append(f"{placeholder.ai_instruction}")
            prompt_parts.append("")

        # Add output format reminder
        prompt_parts.extend([
            "",
            "## è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š",
            "```json",
            "{",
            '  "slides": [',
        ])

        # Generate expected structure
        slide_examples = []
        for slide in template.slides:
            ai_tokens = [ph.token for ph in slide.placeholders if ph.ai_generate]
            if ai_tokens:
                tokens_str = ", ".join(f'"{t}": "..."' for t in ai_tokens)
                slide_examples.append(f'    {{"slide_key": "{slide.slide_key}", "placeholders": {{{tokens_str}}}}}')

        prompt_parts.append(",\n".join(slide_examples))
        prompt_parts.extend([
            "  ],",
            "}",
            "```",
        ])

        return "\n".join(prompt_parts)

    def _build_user_prompt_for_slides(
        self,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2,
        slide_keys: List[str],
        batch_index: int = 0,
        total_batches: int = 1,
    ) -> str:
        """Build user prompt for a subset of slides (for batched generation).

        Args:
            tenant_input: Raw tenant input data
            template: Template descriptor
            slide_keys: List of slide_keys to include in this batch
            batch_index: Current batch index (0-based)
            total_batches: Total number of batches

        Returns:
            User prompt string for the specified slides
        """
        tenant = tenant_input.get("tenant", {})
        period = tenant_input.get("period", {})

        prompt_parts = [
            "## å®¢æˆ·ä¿¡æ¯",
            f"- å®¢æˆ·åç§°ï¼š{tenant.get('name', '')}",
            f"- è¡Œä¸šï¼š{tenant.get('industry', '')}",
            f"- åœ°åŒºï¼š{tenant.get('region', '')}",
            "",
            "## æŠ¥å‘Šå‘¨æœŸ",
            f"- å¼€å§‹æ—¥æœŸï¼š{period.get('start', '')}",
            f"- ç»“æŸæ—¥æœŸï¼š{period.get('end', '')}",
            "",
            "## å®‰å…¨æ•°æ®",
            "```json",
            json.dumps(tenant_input.raw, ensure_ascii=False, indent=2),
            "```",
            "",
        ]

        # Add batch info if there are multiple batches
        if total_batches > 1:
            prompt_parts.extend([
                f"## æ‰¹æ¬¡ä¿¡æ¯",
                f"è¿™æ˜¯ç¬¬ {batch_index + 1}/{total_batches} æ‰¹æ¬¡ï¼Œè¯·åªç”Ÿæˆä»¥ä¸‹æŒ‡å®šslidesçš„å†…å®¹ã€‚",
                "",
            ])

        prompt_parts.extend([
            "## éœ€è¦ç”Ÿæˆçš„å†…å®¹",
            "",
        ])

        # Get AI placeholders only for specified slides
        ai_placeholders = template.get_ai_placeholders()
        current_slide = None

        for slide_key, token, placeholder in ai_placeholders:
            # Skip slides not in this batch
            if slide_key not in slide_keys:
                continue

            if slide_key != current_slide:
                # Find slide title
                for slide in template.slides:
                    if slide.slide_key == slide_key:
                        prompt_parts.append(f"### Slide: {slide.title} ({slide_key})")
                        break
                current_slide = slide_key

            # Add placeholder instruction
            constraints = []
            if placeholder.max_length:
                constraints.append(f"æœ€å¤š{placeholder.max_length}å­—")
            if placeholder.max_items:
                constraints.append(f"æœ€å¤š{placeholder.max_items}æ¡")
            if placeholder.max_chars_per_item:
                constraints.append(f"æ¯æ¡æœ€å¤š{placeholder.max_chars_per_item}å­—")

            constraint_str = f" ({', '.join(constraints)})" if constraints else ""

            prompt_parts.append(f"\n**{token}**{constraint_str}")
            prompt_parts.append(f"{placeholder.ai_instruction}")
            prompt_parts.append("")

        # Add output format reminder
        prompt_parts.extend([
            "",
            "## è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š",
            "```json",
            "{",
            '  "slides": [',
        ])

        # Generate expected structure for only the specified slides
        slide_examples = []
        for slide in template.slides:
            if slide.slide_key not in slide_keys:
                continue
            ai_tokens = [ph.token for ph in slide.placeholders if ph.ai_generate]
            if ai_tokens:
                tokens_str = ", ".join(f'"{t}": "..."' for t in ai_tokens)
                slide_examples.append(f'    {{"slide_key": "{slide.slide_key}", "placeholders": {{{tokens_str}}}}}')

        prompt_parts.append(",\n".join(slide_examples))
        prompt_parts.extend([
            "  ]",
            "}",
            "```",
        ])

        return "\n".join(prompt_parts)

    def _estimate_prompt_tokens(self, text: str) -> int:
        """Estimate token count for a text string.

        For Chinese text, roughly 1.5-2 characters per token.
        For English/code, roughly 4 characters per token.
        We use a conservative estimate of 2 characters per token for mixed content.
        """
        return len(text) // 2

    def _estimate_slide_instruction_size(
        self,
        slide_key: str,
        template: TemplateDescriptorV2,
    ) -> int:
        """Estimate the instruction size for a slide's AI placeholders."""
        size = 0
        for slide in template.slides:
            if slide.slide_key == slide_key:
                # Add slide header
                size += len(f"### Slide: {slide.title} ({slide_key})\n")
                for ph in slide.placeholders:
                    if ph.ai_generate and ph.ai_instruction:
                        size += len(f"\n**{ph.token}**\n")
                        size += len(ph.ai_instruction or "")
                        size += 50  # constraints and formatting overhead
                break
        return size

    def _get_smart_slide_batches(
        self,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2,
        max_tokens_per_batch: int = 6000,
    ) -> List[List[str]]:
        """Split slides into batches based on estimated token count.

        This method intelligently groups slides to keep each batch under
        the token limit, avoiding API timeouts.

        Args:
            tenant_input: Raw tenant input data (needed for base prompt size)
            template: Template descriptor
            max_tokens_per_batch: Maximum estimated tokens per batch

        Returns:
            List of batches, where each batch is a list of slide_keys
        """
        # Calculate base prompt size (customer info + input data)
        # This is constant across all batches
        tenant = tenant_input.get("tenant", {})
        period = tenant_input.get("period", {})
        base_prompt = "\n".join([
            "## å®¢æˆ·ä¿¡æ¯",
            f"- å®¢æˆ·åç§°ï¼š{tenant.get('name', '')}",
            f"- è¡Œä¸šï¼š{tenant.get('industry', '')}",
            f"- åœ°åŒºï¼š{tenant.get('region', '')}",
            "",
            "## æŠ¥å‘Šå‘¨æœŸ",
            f"- å¼€å§‹æ—¥æœŸï¼š{period.get('start', '')}",
            f"- ç»“æŸæ—¥æœŸï¼š{period.get('end', '')}",
            "",
            "## å®‰å…¨æ•°æ®",
            "```json",
            json.dumps(tenant_input.raw, ensure_ascii=False, indent=2),
            "```",
        ])
        base_tokens = self._estimate_prompt_tokens(base_prompt)

        # Reserve tokens for JSON output format instructions (~500 tokens)
        format_overhead = 500

        # Available tokens for slide instructions per batch
        available_tokens = max_tokens_per_batch - base_tokens - format_overhead

        logger.info(f"ğŸ“Š Batch sizing: base={base_tokens} tokens, available={available_tokens} tokens/batch")

        # Calculate instruction size for each slide with AI placeholders
        slide_sizes: List[tuple] = []  # (slide_key, estimated_tokens)
        for slide in template.slides:
            ai_count = sum(1 for ph in slide.placeholders if ph.ai_generate)
            if ai_count == 0:
                continue
            instruction_size = self._estimate_slide_instruction_size(slide.slide_key, template)
            estimated_tokens = self._estimate_prompt_tokens(" " * instruction_size)
            slide_sizes.append((slide.slide_key, estimated_tokens))

        # If total is small enough, no batching needed
        total_instruction_tokens = sum(t for _, t in slide_sizes)
        if total_instruction_tokens <= available_tokens:
            logger.info(f"ğŸ“¦ No batching needed: {total_instruction_tokens} tokens fits in {available_tokens}")
            return [[s for s, _ in slide_sizes]]

        # Greedy batching: add slides until we exceed the limit
        batches: List[List[str]] = []
        current_batch: List[str] = []
        current_tokens = 0

        for slide_key, tokens in slide_sizes:
            # If adding this slide would exceed the limit, start a new batch
            if current_tokens + tokens > available_tokens and current_batch:
                batches.append(current_batch)
                logger.info(f"  Batch {len(batches)}: {current_batch} (~{current_tokens} tokens)")
                current_batch = []
                current_tokens = 0

            current_batch.append(slide_key)
            current_tokens += tokens

        # Don't forget the last batch
        if current_batch:
            batches.append(current_batch)
            logger.info(f"  Batch {len(batches)}: {current_batch} (~{current_tokens} tokens)")

        return batches

    def _generate_ai_content_in_batches(
        self,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2,
        max_tokens_per_batch: int = 6000,
    ) -> Dict[str, Dict[str, Any]]:
        """Generate AI content in batches to avoid timeout issues.

        Args:
            tenant_input: Raw tenant input data
            template: Template descriptor
            max_tokens_per_batch: Maximum estimated tokens per API call

        Returns:
            Dict[slide_key, Dict[token, value]] with all AI-generated content
        """
        batches = self._get_smart_slide_batches(tenant_input, template, max_tokens_per_batch)
        total_batches = len(batches)

        if total_batches <= 1:
            # No need for batching, use original method
            logger.info("ğŸ“¦ Single batch - using standard generation")
            system_prompt = self._build_system_prompt(template)
            user_prompt = self._build_user_prompt(tenant_input, template)
            response = self._call_openai_with_retry(system_prompt, user_prompt)
            return self._parse_llm_response(response, template)

        logger.info(f"ğŸ“¦ Smart batching: splitting into {total_batches} batches")

        all_ai_placeholders: Dict[str, Dict[str, Any]] = {}
        system_prompt = self._build_system_prompt(template)

        for i, batch_slide_keys in enumerate(batches):
            logger.info(f"ğŸ”„ Processing batch {i + 1}/{total_batches}: slides {batch_slide_keys}")

            user_prompt = self._build_user_prompt_for_slides(
                tenant_input,
                template,
                batch_slide_keys,
                batch_index=i,
                total_batches=total_batches,
            )

            prompt_tokens = self._estimate_prompt_tokens(user_prompt)
            logger.info(f"   Batch prompt size: ~{prompt_tokens} tokens")

            response = self._call_openai_with_retry(system_prompt, user_prompt)
            batch_placeholders = self._parse_llm_response(response, template)

            # Merge batch results
            for slide_key, tokens in batch_placeholders.items():
                if slide_key not in all_ai_placeholders:
                    all_ai_placeholders[slide_key] = {}
                all_ai_placeholders[slide_key].update(tokens)

            logger.info(f"âœ… Batch {i + 1}/{total_batches} completed")

        return all_ai_placeholders

    def _call_openai_with_retry(
        self,
        system_prompt: str,
        user_prompt: str,
        max_retries: int = 3,
        retry_delay: float = 2.0,
    ) -> str:
        """Call OpenAI API with retry logic."""
        if not self.client:
            raise LLMGenerationError("OpenAI client is not initialized. Enable LLM in settings.")

        logger.info("=" * 80)
        logger.info("CALLING OPENAI API (V2)")
        logger.info(f"Model: {config.settings.openai_model}")
        logger.info(f"System prompt length: {len(system_prompt)} chars")
        logger.info(f"User prompt length: {len(user_prompt)} chars")
        logger.info("=" * 80)

        last_error = None
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ API Call Attempt {attempt + 1}/{max_retries}")

                response = self.client.chat.completions.create(
                    model=config.settings.openai_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    response_format={"type": "json_object"},
                )

                content = response.choices[0].message.content
                if not content:
                    raise LLMGenerationError("OpenAI returned empty response")

                logger.info("=" * 80)
                logger.info("âœ… OPENAI API CALL SUCCESSFUL")
                logger.info(f"Total tokens used: {response.usage.total_tokens}")
                logger.info(f"Response length: {len(content)} chars")
                logger.info("=" * 80)
                return content

            except RateLimitError as e:
                last_error = e
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    logger.warning(f"âš ï¸ Rate limit hit, waiting {wait_time}s...")
                    time.sleep(wait_time)

            except APIConnectionError as e:
                last_error = e
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ Connection error: {e}, retrying...")
                    time.sleep(retry_delay)

            except APIError as e:
                last_error = e
                logger.error(f"âŒ OpenAI API error: {e}")
                break

            except Exception as e:
                last_error = e
                logger.error(f"âŒ Unexpected error: {e}")
                break

        raise LLMGenerationError(f"Failed after {max_retries} attempts: {last_error}") from last_error

    def _sanitize_llm_json(self, content: str) -> str:
        """Clean up LLM response for JSON parsing."""
        if not content:
            return content

        text = content.strip()

        # Remove markdown code fences
        fenced_match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", text, re.IGNORECASE)
        if fenced_match:
            text = fenced_match.group(1).strip()

        # Extract JSON object
        first_brace = text.find("{")
        last_brace = text.rfind("}")
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            return text[first_brace:last_brace + 1].strip()

        return text

    def _parse_llm_response(
        self,
        response: str,
        template: TemplateDescriptorV2
    ) -> Dict[str, Dict[str, Any]]:
        """Parse LLM response into slide placeholders.

        Returns:
            Dict[slide_key, Dict[token, value]]
        """
        cleaned = self._sanitize_llm_json(response)

        try:
            data = json.loads(cleaned)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {e}")
            logger.error(f"Response: {cleaned[:500]}...")
            raise LLMGenerationError(f"Invalid JSON from LLM: {e}") from e

        result: Dict[str, Dict[str, Any]] = {}

        if "slides" not in data:
            raise LLMGenerationError("Response missing 'slides' field")

        for slide_data in data["slides"]:
            slide_key = slide_data.get("slide_key")
            placeholders = slide_data.get("placeholders", {})

            if slide_key:
                result[slide_key] = placeholders

        return result

    def _validate_key_numbers(
        self,
        slidespec: SlideSpecV2,
        tenant_input: TenantInput,
        template: TemplateDescriptorV2
    ) -> List[str]:
        """Validate key numerical fields match input data.

        Returns:
            List of validation error messages (empty if all valid)
        """
        errors = []
        validation_fields = template.get_validation_fields()

        # Computed values
        incidents = tenant_input.get("incidents", []) or []
        computed = {
            "incidents_count": len(incidents),
            "incidents_high_count": len([i for i in incidents if i.get("severity") == "high"]),
        }

        for token, field_path in validation_fields.items():
            # Get expected value
            if field_path in computed:
                expected = computed[field_path]
            else:
                expected = self._get_nested(tenant_input, field_path)

            if expected is None:
                continue

            # Find actual value in slidespec
            for slide in slidespec.slides:
                if token in slide.placeholders:
                    actual = slide.placeholders[token]

                    # Extract number from string if needed
                    if isinstance(actual, str):
                        numbers = re.findall(r'[\d.]+', actual)
                        if numbers:
                            try:
                                actual = float(numbers[0]) if '.' in numbers[0] else int(numbers[0])
                            except ValueError:
                                pass

                    # Compare
                    if isinstance(expected, (int, float)) and isinstance(actual, (int, float)):
                        if abs(expected - actual) > 0.01:
                            errors.append(f"{token}: expected {expected}, got {actual}")

        return errors

    def generate_slidespec_v2(
        self,
        tenant_input: TenantInput,
        template_id: str,
        use_mock: bool = False,
    ) -> SlideSpecV2:
        """Generate SlideSpec for V2 template using AI.

        This is the main entry point for V2 generation.

        Args:
            tenant_input: Raw tenant input data
            template_id: V2 template ID
            use_mock: Whether to force mock/fallback generation

        Returns:
            SlideSpecV2 with all placeholders filled
        """
        logger.info(f"ğŸ¯ Generating V2 slidespec for template: {template_id}, use_mock={use_mock}")

        # Load V2 template descriptor
        template = self.template_repo.get_descriptor_v2(template_id)

        # Create empty slidespec structure
        slide_keys = [(s.slide_no, s.slide_key) for s in template.slides]
        slidespec = create_empty_slidespec_v2(template_id, slide_keys)

        # Step 1: Extract data placeholders (non-AI)
        logger.info("ğŸ“Š Extracting data placeholders...")
        data_placeholders = self._extract_data_placeholders(tenant_input, template)

        for slide_key, tokens in data_placeholders.items():
            slide = slidespec.get_slide(slide_key)
            if slide:
                slide.placeholders.update(tokens)

        # Step 2: Generate AI placeholders
        if config.settings.enable_llm and not use_mock:
            logger.info("ğŸ¤– Generating AI content...")
            try:
                # Use smart batched generation to avoid timeout issues
                # Batching is based on estimated token count, not hardcoded limits
                ai_placeholders = self._generate_ai_content_in_batches(
                    tenant_input,
                    template,
                )

                # Merge AI content
                for slide_key, tokens in ai_placeholders.items():
                    slide = slidespec.get_slide(slide_key)
                    if slide:
                        slide.placeholders.update(tokens)

                # Validate key numbers
                errors = self._validate_key_numbers(slidespec, tenant_input, template)
                if errors:
                    logger.warning(f"âš ï¸ Validation warnings: {errors}")

            except LLMGenerationError as e:
                logger.error(f"âŒ AI generation failed: {e}")
                logger.warning("âš ï¸ Falling back to placeholder text")
                self._fill_ai_placeholders_with_fallback(slidespec, template)
        else:
            logger.info(f"ğŸ“ {'Using mock mode' if use_mock else 'LLM disabled'}, using fallback content")
            self._fill_ai_placeholders_with_fallback(slidespec, template)

        logger.info(f"âœ… V2 slidespec generation complete: {len(slidespec.slides)} slides")
        return slidespec

    def _fill_ai_placeholders_with_fallback(
        self,
        slidespec: SlideSpecV2,
        template: TemplateDescriptorV2,
    ) -> None:
        """Fill AI placeholders with fallback text when LLM is unavailable."""
        for slide_key, token, placeholder in template.get_ai_placeholders():
            slide = slidespec.get_slide(slide_key)
            if slide and token not in slide.placeholders:
                if placeholder.type == "bullet_list":
                    slide.placeholders[token] = "â€¢ [AIç”Ÿæˆå†…å®¹å ä½]\nâ€¢ [è¯·å¯ç”¨LLMä»¥ç”Ÿæˆå®é™…å†…å®¹]"
                else:
                    slide.placeholders[token] = f"[{token}: AIç”Ÿæˆå†…å®¹å ä½]"


# ============================================================================
# Legacy V1 Orchestrator (kept for backward compatibility)
# ============================================================================

class LLMOrchestrator:
    """V1 Orchestrator - Legacy implementation for V1 templates."""

    def __init__(self, template_repo: Optional[TemplateRepository] = None):
        self.template_repo = template_repo or TemplateRepository()
        self.client: Optional[OpenAI] = None

        if config.settings.enable_llm:
            try:
                client_kwargs = {"api_key": config.settings.openai_api_key}
                if config.settings.openai_base_url:
                    client_kwargs["base_url"] = config.settings.openai_base_url
                self.client = OpenAI(**client_kwargs)
            except Exception as e:
                raise LLMGenerationError(f"OpenAI client initialization failed: {e}") from e

    def _load_mock_slidespec(self, input_id: str, template_id: str) -> SlideSpec:
        """Load mock slidespec for fallback."""
        audience = "management" if "management" in template_id else "technical"
        mock_file = f"{input_id}_{audience}_mock_slidespec.json"
        path = config.MOCK_OUTPUTS_DIR / mock_file
        if not path.exists():
            raise MockOutputNotFound(f"Mock slidespec {mock_file} not found")
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return SlideSpec.model_validate(data)

    def generate_slidespec(
        self,
        input_id: str,
        template_id: str,
        prepared: Any,  # DataPrepResult
        use_mock: bool = False,
    ) -> SlideSpec:
        """Generate slidespec using V1 logic (legacy)."""
        logger.info(f"ğŸ¯ Generating V1 slidespec for {input_id}/{template_id}")

        if use_mock:
            try:
                return self._load_mock_slidespec(input_id, template_id)
            except MockOutputNotFound:
                logger.warning("Mock not found, using deterministic generation")

        # Deterministic fallback
        slides = []
        template = self.template_repo.get_descriptor(template_id)
        for slide in template.slides:
            data = prepared.slide_inputs.get(slide.slide_key, {})
            slides.append(SlideSpecItem(
                slide_no=slide.slide_no,
                slide_key=slide.slide_key,
                data=data,
            ))

        return SlideSpec(template_id=template_id, slides=slides)

    def rewrite_slide(
        self,
        slide_spec: SlideSpec,
        slide_key: str,
        new_content: Dict[str, Any],
    ) -> SlideSpec:
        """Update a slide's data with new content."""
        updated_slides = []
        for slide in slide_spec.slides:
            if slide.slide_key == slide_key:
                updated_slides.append(SlideSpecItem(
                    slide_no=slide.slide_no,
                    slide_key=slide.slide_key,
                    data={**slide.data, **new_content},
                ))
            else:
                updated_slides.append(slide)
        return SlideSpec(template_id=slide_spec.template_id, slides=updated_slides)
